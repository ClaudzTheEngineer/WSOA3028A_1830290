<!DOCTYPE html>
<html id="top">

<head>
    <link href="../css/stylesheet.css" rel="stylesheet">
    <script src="../js/nav.js"></script>
    <meta charset="UTF-8">
    <meta name="desciption" content="Pandora's Box">
    <meta name="author" content="Claudio Surmon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        Pandora's Box
    </title>
</head>

<body onload="insertElements(2,true)">

    <header></header>

    <nav></nav>
    <article>
        <h1>
            Pandora's Box
        </h1>
        <p id="info">
            by Claudio Surmon
        </p>

        <figure>
            <img src="../images/X-47B_operating_in_the_Atlantic_Test_Range_(modified).jpg" alt="PlaceHolder"
                width="800px">
            <figcaption>
                Placeholder
            </figcaption>
        </figure>

        <p>
            Machine learning in recent years has opened a pandora’s box, one that can never be closed again. Pandora’s
            box is a mythological artefact which has become synonymous with a source of large amounts of unexpected
            trouble and strife. So, what does this have to do with machine learning.
            <br><br>
            Although machine learning has the potential to create wonders of technology, with promises of a better life
            for all, the opposite potential has reared its ugly head. Recently a creator at the forefront of computer
            vision research has decided to cease his research due to the military and ethical concerns. However, do
            these researchers have the luxury of just stopping this research?

        </p>


        <!-- Tweet embedding -->
        <blockquote class="twitter-tweet tw-align-center" data-lang="en">
            <p lang="en" dir="ltr">This is huge.<br><br>The creator of the YOLO algorithms, which (along with SSD) set
                much of the path of modern object detection, has stopped doing any computer vision research due to
                ethical concerns.<br><br>I&#39;ve never seen anything quite like this before. <a
                    href="https://t.co/jzu1p4my5V">https://t.co/jzu1p4my5V</a></p>&mdash; Jeremy #Masks4All Howard
            (@jeremyphoward) <a
                href="https://twitter.com/jeremyphoward/status/1230610470991589376?ref_src=twsrc%5Etfw">February 20,
                2020</a>
        </blockquote>
        <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

        <p>
            I don’t think these inventors, for the most part, envisioned the long-term effects of the technology they
            were working on. When these people were sitting up late at night, caffeinated in the empty computer science
            lab, they were focused on the wonder they were creating. A camera that can recognise what it sees, a program
            that find patterns in medical history and other great ideas. But any great idea can be perverted to serve
            ulterior motives, especially with regards to war.
        </p>
        <h2>Nuclear Warfare</h2>
        <p>
            Something similar occurred with the atomic bomb and nuclear technology at large. Even though the cold war
            happened and the general populous disregard nuclear arms, thousands of active nuclear warheads stand at the
            ready. Einstein once said, “Had I known that the Germans would not succeed in producing an atomic bomb, I
            would not have lifted a finger.” However, we can never go back to a pre-nuclear age, all it takes is for one
            rogue nation to recognise its power and threaten to use it. Without the threat of mutual annihilation these
            rogue states would quickly wipe out their opponents.
            <br><br>
            Warfare as an ethic debate is ludicrously grey, trying to justify when to kill other people, destroy
            property and infrastructure. During a time of war, the person in charge has numerous decisions to make with
            one of the primary goals to be protect their own people, and to this extent a country can and will go to
            extreme lengths to do just that.
            <br><br>
            For instance, a country like Switzerland where they have not fought in a conflict since 1815, still has
            compulsory military service for all male Swiss citizens with women serving voluntarily. The country also has
            roughly 26 000 maintained bunkers and fortifications, with a lot of strategic locations, including bridges,
            tunnels and airstrips, designed to be self-detonated at a moment’s notice. The Swiss' somewhat apocalyptic
            defence measures ensure the safety of its citizens no matter the scenario.
        </p>
        <h2>
            AI in militaries
        </h2>
        <p>
            For use in defence, artificial intelligence and automation is more justifiable, helping to minimise
            casualties. The use of which allows for the employment of more active forms of defence. For instance, the
            Iron Dome.
            <br><br>
            The iron dome is an all-weather mobile air defence system created by Rafael in the mid noughties, it has
            been in service in Israel since 2011. This system is designed to intercept and destroy short-range rockets
            and artillery rounds which would impact in a populated Israeli area. By 2014 the system had successfully
            shot down 1200 rockets with a reported success rate of 90%. The system was also helped protect NATO forces
            in Iraq and Afghanistan. An example of the system in action can be seen in the video below.

        </p>
        <iframe width="560" height="315" src="https://www.youtube-nocookie.com/embed/hcNE5VHLldY" frameborder="0"
            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
        <p>
            But where do you draw the line, it is right and good to defend but what about attacking?
            During a meeting at the UN, 82 countries met to discuss the banning of automated weaponry, although most
            countries agreed that LAWS (Lethal Autonomous Weapon Systems) should be banned, 5 countries explicitly
            rejected the movement, those being France, Israel, Russia, the UK and the USA. Is it surprising that 3 of
            the 5 most powerful militaries do not want these powerful weapons banned?
            <br><br>
            In a fight between a man and a machine, 9 times out of 10 the machine will win, able to aim better,
            calculate trajectories faster and more accurately, It never tires, never get distracted and is able to work
            in horrific conditions. When speaking with a former member of the South African defence force, he stated
            that “militaries will always try to get the upper hand on each other. A drone has the capabilities to pull
            higher speed manoeuvres in a dog fight, be more aware of its surroundings and be able to focus on multiple
            things simultaneously, all while being expendable.” An example of one of this class of drone would be the
            Northrop Grumman X-47B (pictured above) for use on aircraft carriers.
            <br><br>
            So, we opened Pandora’s box and found that we can protect our citizens, but we also found the capability to
            create machines that can mindlessly slaughter people.
            <br><br>
            In conclusion, we can never go back, the technology has been created, and if you as a country choose not to
            use these tools, your enemy will, and it’s a matter of when they will win, not if.

        </p>
        <h2>
            References
        </h2>
        <p>

        </p>

    </article>
    <nav class="blogNav"></nav>
    <footer> </footer>
</body>

</html>