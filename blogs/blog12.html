<!DOCTYPE html>
<html id="top">

<head>
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
        integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk" crossorigin="anonymous">
    <link href="/WSOA3028A_1830290/css/bootstrapStylesheet.css" rel="stylesheet">
    <script src="../js/nav.js"></script>
    <meta charset="UTF-8">
    <meta name="desciption" content="Template">
    <meta name="author" content="Claudio Surmon">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
        The Black, the White and the Grey
    </title>
</head>

<body onload="insertElements(2,true)">

    <header class="container"></header>

    <nav class="navbar" id="navbar"></nav>
    <article class="container">
        <h1>
            The Black, the White and the Grey
        </h1>
        <p id="info">
            by Claudio Surmon
        </p>
        <figure>
            <img src="../images/Artificial_Intelligence.png" alt="Wireframe of a robot holding the letters AI">
            <figcaption>
                Licensed under the Creative Commons Attribution-Share Alike
                4.0 International, credit: Vaishali1840265
            </figcaption>
        </figure>
        <p>
            Artificial Intelligence in this context, is a system that describes a situation present in a data set (like
            photos) the theory and development of computer systems able to perform tasks normally requiring human
            intelligence, such as visual perception, speech recognition, decision-making, and translation between
            languages. Since AI is based on data, you need good data to get reliable predictions. However, an AI lacks
            something humans have, context. This creates a requirement that the data fed into the AI is clean. Clean in
            this case refers to two things, good data coverage and completeness of data. Since good data coverage is
            often difficult to achieve, larger data sets are often used as a proxy for good data coverage. Incomplete
            data is data that doesn’t contain enough information about the world to replace
            human knowledge. This can be caused by the lack of availability of better data, incorrect assumptions of
            machine learning and its outcomes or improper execution of program design.
            <br><br>
            Recently there was an AI that was used to differentiate between dogs and wolves. After training, the system
            had a very good success rate, however, when the system saw huskies in the snow, it identified them as
            wolves. This was
            because all the photos of wolves had been on the snow, while the dogs were on grass or similar. So the AI
            differentiated between the photos based on the presence of snow.
            <br><br>
            Lets imagine you have an AI that scans boxes leaving your factory for damage and would flag any box that had
            damage on it. If the creator of this AI is careless with the training data, the system may flag any box that
            is not rectangular, or not see the damage on a darker box. By not making sure the data you have provided is
            a large amount of clean and complete data, you run the risk of creating programs that create false
            positives.
            <br><br>
            In these cases the outcomes from these mistakes would be harmless, annoying sure but mostly harmless. What
            if the biased system oversaw something far more important, possibly where people’s lives and well-being is
            on the line?
            <br><br>
            When automated systems are integrated into key parts of society (banks, job applications, etc) extra care
            needs to be taken to prevent racial or other forms of prejudice being perpetuated. However
            biases reinforce biases. Any sort of predujice that the system enforces could and would be devastating. If
            an automated loan application denied your loan solely on skin colour, or a job application gets rejected
            immediately because of your gender, then any attempts to right the predujice within our society would be
            denied before it started.
            <br><br>
            The pattern shows itself repeatedly, reinforcing bad habits within society. It is our duty to train systems
            not to do that. But for that to happen we would need to train it out of society first.
        </p>
        <h2>References</h2>
        <p>
            <a
                href="https://hackernoon.com/dogs-wolves-data-science-and-why-machines-must-learn-like-humans-do-41c43bc7f982">Hackernoon</a><br>
            <a href="https://www.coursera.org/learn/machine-learning-business-professionals">Machine Learning for
                Business Professionals by Google Cloud</a><br>
        </p>
    </article>
    <nav class="blogNav container"></nav>
    <footer class="container"> </footer>
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
        integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN"
        crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.12.9/umd/popper.min.js"
        integrity="sha384-ApNbgh9B+Y1QKtv3Rn7W3mgPxhU9K/ScQsAP7hUibX39j7fakFPskvXusvfa0b4Q"
        crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/js/bootstrap.min.js"
        integrity="sha384-JZR6Spejh4U02d8jOt6vLEHfe/JQGiRRSQQxSfFWpi1MquVdAyjUar5+76PVCmYl"
        crossorigin="anonymous"></script>
</body>

</html>